{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847c1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a8110",
   "metadata": {},
   "outputs": [],
   "source": [
    " train_data = pd.read_csv(\"../input/digit-recognizer/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa076cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_data_unscaled = train_data.drop(['label'], axis=1)\n",
    "model_train_label = train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2aac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(model_train_data_unscaled.loc[10]).reshape(28, 28), cmap='Greys')\n",
    "print(model_train_label[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no missing value and there is very little scope of feature engineering; so we will only do scaling of pixel values to bring them within 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "std_scaler = MinMaxScaler()\n",
    "model_train_data = std_scaler.fit_transform(model_train_data_unscaled.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f088f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve\n",
    "\n",
    "def model_def(model, model_name, m_train_data, m_train_label):\n",
    "    model.fit(m_train_data, m_train_label)\n",
    "    s = \"predict_\"\n",
    "    p = s + model_name\n",
    "    p = model.predict(m_train_data)\n",
    "    cm = confusion_matrix(m_train_label, p)\n",
    "    precision = np.diag(cm)/np.sum(cm, axis=0)\n",
    "    recall    = np.diag(cm)/np.sum(cm, axis=1)\n",
    "    F1 = 2 * np.mean(precision) * np.mean(recall)/(np.mean(precision) + np.mean(recall))\n",
    "    cv_score = cross_val_score(model, m_train_data, m_train_label, cv=3, scoring='accuracy')\n",
    "    print(\"Precision Is      :\", np.mean(precision))\n",
    "    print(\"Recall Is         :\", np.mean(recall))\n",
    "    print(\"F1 Score IS       :\", F1)\n",
    "    print(\"Mean CV Score     :\", cv_score.mean())\n",
    "    print(\"Std Dev CV Score  :\", cv_score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using softmax regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "softmax = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial', C=0.05)\n",
    "model_def(softmax, \"softmax\", model_train_data, model_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will apply PCA and which is one of the main dimensionality reduction technique. PCA stands for Principal Component Analysis and here the columns across which the data has maximum variability are retained; \n",
    "#so we can drop few columns without losing significant amount of insights.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 200)\n",
    "model_train_data2D = pca.fit_transform(model_train_data)\n",
    "print(\"Explained Variance Ratio:\", np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So you can see if we use only 200 columns out of 784 columns we are able to explain 96.61% variability of the data.\n",
    "#We will use this updated dataset on Polynomial Kernel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly6 = SVC(C=2, kernel='poly', degree=3, gamma='auto', random_state=42)\n",
    "model_def(poly6, \"poly6\", model_train_data2D, model_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will apply ANN to solve the challenge. Since we want to use the standard function to display model metrics like CV score we need to build an wrapper\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    classifier = Sequential([Dense(128, kernel_initializer='random_uniform', activation='relu', input_shape=(200,)),\n",
    "                             Dropout(rate=0.2),\n",
    "                             Dense(128, kernel_initializer='random_uniform', activation='relu'),\n",
    "                             Dropout(rate=0.2),\n",
    "                             Dense(10, kernel_initializer='random_uniform', activation='softmax')])\n",
    "    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "ANN_classifier = KerasClassifier(build_fn=build_classifier, batch_size=100, epochs=20)\n",
    "ANN_classifier.fit(model_train_data2D, model_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(ANN_classifier, model_train_data2D, model_train_label, cv=5, scoring='accuracy')\n",
    "print(\"Mean CV Score Is:\", cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean CV Score Is:\", cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../input/digit-recognizer/test.csv\")\n",
    "\n",
    "model_test_data = std_scaler.transform(test_data.astype(np.float64))\n",
    "model_test_data2D = pca.transform(model_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdde53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_poly6 = poly6.predict(model_test_data2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c412bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_softmax = softmax.predict(model_test_data)\n",
    "predict_test_ANN = ANN_classifier.predict(model_test_data2D)\n",
    "\n",
    "Id = DataFrame(np.arange(1,28001))\n",
    "Id.columns = ['ImageId']\n",
    "\n",
    "prediction = DataFrame(predict_test_poly6)\n",
    "prediction.columns = ['Label']\n",
    "\n",
    "result = pd.concat([Id, prediction], axis=1)\n",
    "result.to_csv(\"Submission_Poly.csv\", index=False)\n",
    "\n",
    "Id = DataFrame(np.arange(1,28001))\n",
    "Id.columns = ['ImageId']\n",
    "\n",
    "prediction = DataFrame(predict_test_softmax)\n",
    "prediction.columns = ['Label']\n",
    "\n",
    "result = pd.concat([Id, prediction], axis=1)\n",
    "result.to_csv(\"Submission_Softmax.csv\", index=False)\n",
    "\n",
    "Id = DataFrame(np.arange(1,28001))\n",
    "Id.columns = ['ImageId']\n",
    "\n",
    "prediction = DataFrame(predict_test_ANN)\n",
    "prediction.columns = ['Label']\n",
    "\n",
    "result = pd.concat([Id, prediction], axis=1)\n",
    "result.to_csv(\"Submission_ANN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b503edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will apply Convolutional Neural Network(CNN) techniques on the original data.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the data\n",
    "X_train = np.array(model_train_data_unscaled).reshape(42000, 28, 28, 1)\n",
    "y_train = model_train_label\n",
    "X_test = np.array(test_data).reshape(28000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = Sequential([Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=(28,28,1),\n",
    "                            padding='valid', activation='relu'),\n",
    "                         MaxPooling2D(pool_size=(2, 2)),\n",
    "                         Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            padding='valid', activation='relu'),\n",
    "                         MaxPooling2D(pool_size=(2, 2)),\n",
    "                         Flatten(),\n",
    "                         Dense(128, kernel_initializer='random_uniform', activation='relu'),\n",
    "                         Dropout(rate=0.2),\n",
    "                         Dense(128, kernel_initializer='random_uniform', activation='relu'),\n",
    "                         Dropout(rate=0.2),\n",
    "                         Dense(10, kernel_initializer='random_uniform', activation='softmax')])\n",
    "\n",
    "CNN_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "CNN_model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = CNN_model.predict(X_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec19351",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_CNN = np.argmax(y_test, axis=1)\n",
    "plt.imshow(np.array(test_data.loc[0]).reshape(28, 28), cmap='Greys')\n",
    "print(predict_test_CNN[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de3b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ae137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
